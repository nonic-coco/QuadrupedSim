% Quadruped Robot -- DDPG Neural Network Setup Script
% Copyright 2019-2022 The MathWorks, Inc.

% Network structure inspired by original 2015 DDPG paper 
% "Continuous Control with Deep Reinforcement Learning", Lillicrap et al.
% https://arxiv.org/pdf/1509.02971.pdf

%% CRITIC
% Create the critic network layers
criticLayerSizes = [400 300];
statePath = [
    featureInputLayer(numObs, Name="observation")
    fullyConnectedLayer(criticLayerSizes(1), ...
            Weights=2/sqrt(numObs)*(rand(criticLayerSizes(1),numObs)-0.5), ...
            Bias=2/sqrt(numObs)*(rand(criticLayerSizes(1),1)-0.5))
    reluLayer()
    fullyConnectedLayer(criticLayerSizes(2), ...
            Name="observationFC", ...
            Weights=2/sqrt(criticLayerSizes(1))*(rand(criticLayerSizes(2),criticLayerSizes(1))-0.5), ... 
            Bias=2/sqrt(criticLayerSizes(1))*(rand(criticLayerSizes(2),1)-0.5))
    ];
actionPath = [
    featureInputLayer(numAct, Name="action")
    fullyConnectedLayer(criticLayerSizes(2), ...
            Name="actionFC", ...
            Weights=2/sqrt(numAct)*(rand(criticLayerSizes(2),numAct)-0.5), ... 
            Bias=2/sqrt(numAct)*(rand(criticLayerSizes(2),1)-0.5))
    ];
commonPath = [
    additionLayer(2, Name="add")
    reluLayer()
    fullyConnectedLayer(1, ...
            Name="criticOutput",...
            Weights=2*5e-3*(rand(1,criticLayerSizes(2))-0.5), ...
            Bias=2*5e-3*(rand(1,1)-0.5))
    ];
% Connect the layer graph
criticNetwork = layerGraph(statePath);
criticNetwork = addLayers(criticNetwork, actionPath);
criticNetwork = addLayers(criticNetwork, commonPath);
criticNetwork = connectLayers(criticNetwork,"observationFC","add/in1");
criticNetwork = connectLayers(criticNetwork,"actionFC","add/in2");

critic = rlQValueFunction(dlnetwork(criticNetwork), ...
    getObservationInfo(env), ...
    getActionInfo(env), ...
    ObservationInputNames="observation", ...
    ActionInputNames="action");

%% ACTOR
% Create the actor network layers
actorLayerSizes = [400 300];
actorNetwork = [
    featureInputLayer(numObs, Name="observation")
    fullyConnectedLayer(actorLayerSizes(1), ...
            Weights=2/sqrt(numObs)*(rand(actorLayerSizes(1),numObs)-0.5), ... 
            Bias=2/sqrt(numObs)*(rand(actorLayerSizes(1),1)-0.5))
    reluLayer()
    fullyConnectedLayer(actorLayerSizes(2), ... 
            Weights=2/sqrt(actorLayerSizes(1))*(rand(actorLayerSizes(2),actorLayerSizes(1))-0.5), ... 
            Bias=2/sqrt(actorLayerSizes(1))*(rand(actorLayerSizes(2),1)-0.5))
    reluLayer()
    fullyConnectedLayer(numAct, ...
            Weights=2*5e-3*(rand(numAct,actorLayerSizes(2))-0.5), ... 
            Bias=2*5e-5*(rand(numAct,1)-0.5))                       
    tanhLayer(Name="actorOut")
    ];
actorNetwork = layerGraph(actorNetwork);

actor = rlContinuousDeterministicActor(dlnetwork(actorNetwork), ...
    getObservationInfo(env), ...
    getActionInfo(env), ...
    ObservationInputNames="observation");
                     
%% VISUALIZING NETWORKS
% Once you have created the networks, you can visualize and modify them 
% interactively with the Deep Network Designer app.
%
% >> deepNetworkDesigner